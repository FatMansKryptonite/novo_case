{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\decorators.py:262: NumbaDeprecationWarning: \u001b[1mnumba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\u001b[0m\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n",
      "c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\visions\\backends\\shared\\nan_handling.py:50: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @nb.jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from file\n",
    "The `trial_classes.xlsx` file has been enriched with additional multiclass labels identifying some characteristics of the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'data'\n",
    "\n",
    "data_dict = {}\n",
    "for file_name in os.listdir(data_directory):\n",
    "    file_path = os.path.join(data_directory, file_name)\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    name = file_name.split('.')[0]\n",
    "    if file_type == 'parquet':\n",
    "        data_dict[name] = pd.read_parquet(file_path)\n",
    "    elif file_type == 'xlsx':\n",
    "        data_dict[name] = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict['country']\n",
    "\n",
    "# Drop columns with more than 90 % nans\n",
    "threshold = 0.9\n",
    "df = df.dropna(thresh=len(df) * (1 - threshold), axis=1)\n",
    "\n",
    "data_dict['country'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict['target']\n",
    "\n",
    "# Calculate target variable\n",
    "efficiency = df['no_of_patients'] / df['enrolment_months']\n",
    "df['efficiency'] = efficiency\n",
    "\n",
    "# Drop rows with nan target value\n",
    "df = df.dropna(subset=['efficiency'])\n",
    "\n",
    "data_dict['target'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict['trial_classes']\n",
    "\n",
    "# One hot encode trial phase\n",
    "df_ohe = pd.get_dummies(df['trial_phase'], drop_first=True)\n",
    "df_ohe = df_ohe.astype(int)\n",
    "df['trial_phase_III'] = df_ohe\n",
    "\n",
    "data_dict['trial_classes'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict['trial_site']\n",
    "\n",
    "df['site_start_date'] = pd.to_datetime(df['site_start_date'])\n",
    "df['site_end_date'] = pd.to_datetime(df['site_end_date'])\n",
    "df['trial_duration_days'] = (df['site_end_date'] - df['site_start_date']).dt.days\n",
    "\n",
    "data_dict['trial_site'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(data_dict['trial_site'], data_dict['country'], on='country_id', how='left')\n",
    "df = pd.merge(df, data_dict['trial_classes'], on='trial_id', how='left')\n",
    "df = pd.merge(df, data_dict['target'], on=['trial_id', 'site_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Make a smaller dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['site_type', 'trial_duration_days', 'nn_region',\n",
    "       'population', 'oecd_pharma_expenditure_per_capita',\n",
    "       'oecd_medical_grads_per_1k', 'oecd_MDs_per_1k',\n",
    "       'oecd_hospital_beds_per_1k', 'oecd_MR_units_per_1m',\n",
    "       'oecd_pte_units_total', 'oecd_hypnotics_sedatives_per_1k',\n",
    "       'oecd_antidepressants_per_1k', 'oecd_nervous_system_drugs_per_1k',\n",
    "       'oecd_perc_pop_insured_by_gov_health',\n",
    "       'oecd_perc_pop_insured_by_priv_health',\n",
    "       'oecd_perc_pop_insured_by_priv_or_gov_health',\n",
    "       'wb_diabetes_prevalence_perc_pop_age_20_to_79',\n",
    "       'who_gho_ncd_paa_prev_insuff_physical_activity',\n",
    "       'who_gho_ncd_paa_prev_insuff_physical_activity_male',\n",
    "       'who_gho_ncd_paa_prev_insuff_physical_activity_female',\n",
    "       'who_gho_mh_4_gov_exp_on_mental_perc_of_total_health',\n",
    "       'who_gho_mh_12_suicide_rate_per100k',\n",
    "       'who_gho_mh_16_beds_in_mental_hospitals_100k',\n",
    "       'who_gho_mh_13_beds_for_mental_in_gen_hospitals_100k',\n",
    "       'who_gho_gdo_q35_est_pop_prev_depression',\n",
    "       'who_gho_whosis_000001_life_expec_at_birth_w',\n",
    "       'who_gho_NCD_BMI_30A_obesity_prevalence_adults',\n",
    "       'who_gho_ghed_oopsche_sha2011_oop_expenditure_pct_current_health_expenditure',\n",
    "       'who_gho_ghed_che_pc_us_sha2011_curr_health_exp_per_capita_usd',\n",
    "       'who_gho_ghed_che_pc_ppp_sha2011_curr_health_exp_per_capita_ppp',\n",
    "       'who_gho_ghed_chegdp_sha2011_curr_health_exp_perc_gdp',\n",
    "       'who_gho_sdgpm25_pm25_concentration_val',\n",
    "       'who_gho_air_90_air_pollution_dalys_val',\n",
    "       'who_gho_hwf_0025_prc_female_medical_doctors', 'minimum_age',\n",
    "       'is_novo_nordisk_trial', 'is_top_20_sponsor', 'Heart Failure',\n",
    "       'Cardiovascular Stability', 'High Consent Emphasis',\n",
    "       'Technology-Enabled Monitoring', 'Weight Monitoring',\n",
    "       'Medication and Treatment History', 'Kidney Function',\n",
    "       'Cardiovascular Events', 'trial_phase_III', 'no_of_patients',\n",
    "       'enrolment_months', 'efficiency']\n",
    "\n",
    "df_training = df[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to be able to clean out `NaN` values but after clearing rows with any `NaN`s 89 rows remain so this is not feasible. Similarly, if we do it on columns we only get 14 columns left. We need to do something smarter.\n",
    "\n",
    "We elect to use a random forest as an initial machine learning approach as it is robust against missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Profile datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset:   0%|          | 0/63 [00:00<?, ?it/s, Describe variable:site_end_date]c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ydata_profiling\\model\\typeset_relations.py:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return not series.astype(\"datetime64\").isna().all()\n",
      "c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ydata_profiling\\model\\typeset_relations.py:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return not series.astype(\"datetime64\").isna().all()\n",
      "Summarize dataset: 100%|██████████| 1364/1364 [02:12<00:00, 10.26it/s, Completed]                                                                                                                                                      \n",
      "Generate report structure: 100%|██████████| 1/1 [00:13<00:00, 13.42s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:23<00:00, 23.65s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "c:\\Users\\IvarCashinEriksson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ydata_profiling\\utils\\dataframe.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={\"index\": \"df_index\"}, inplace=True)\n",
      "Summarize dataset: 100%|██████████| 1284/1284 [02:14<00:00,  9.53it/s, Completed]                                                                                                                                                      \n",
      "Generate report structure: 100%|██████████| 1/1 [00:13<00:00, 13.34s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:22<00:00, 22.49s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs_dict = {'full': df, 'training': df_training}\n",
    "for key, df in dfs_dict.items():\n",
    "    prof = ProfileReport(df)\n",
    "    prof.to_file(output_file=f'profiles/{key}.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
